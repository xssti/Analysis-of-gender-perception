{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d04617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67084ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pandas in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m917.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/shaistasyeda/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=d91123f603d9a7f9861ebaa125b60f4a73b4e564c6d81856254242b961ac0cbb\n",
      "  Stored in directory: /Users/shaistasyeda/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=1c8895817694ad09d7b9c0a7fa7727df6a6cd3c2757e87f9112fe6d61efe0617\n",
      "  Stored in directory: /Users/shaistasyeda/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378e52b-5771-4b51-80f9-29ba0187237d",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc8c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc162c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt\n",
      "Processed 471034 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1901_1950_tokenized.txt\n",
      "Processed 765422 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1901_1950_tokenized.txt\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1851_1900_tokenized.txt\n",
      "Processed 687819 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1851_1900_tokenized.txt\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1971_1990_tokenized.txt\n",
      "Processed 1025994 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1971_1990_tokenized.txt\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1991_2006_tokenized.txt\n",
      "Processed 597305 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1991_2006_tokenized.txt\n",
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1951_1970_tokenized.txt\n",
      "Processed 813361 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1951_1970_tokenized.txt\n",
      "Word2Vec Model training completed\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "# the directory path of tokenized files\n",
    "data_directory = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates'\n",
    "\n",
    "# Initialize an empty list to store sentences\n",
    "sentences = []\n",
    "print(\"here\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Read the tokenized data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_sentences = [line.split() for line in file]\n",
    "\n",
    "        # Add the sentences from this file to the overall list\n",
    "        sentences.extend(file_sentences)\n",
    "        \n",
    "        print(f\"Processed {len(file_sentences)} sentences from {file_path}\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de00433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82c3ab5-740f-4399-970a-97e74961c9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'woman': [('men', 0.5933540463447571), ('female', 0.58524489402771), ('sex', 0.5548138618469238), ('wife', 0.5384922623634338), ('girl', 0.5375543236732483)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2089b056-c227-4dcb-9221-d745c73c9355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'women' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find words similar to a given word\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m similar_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwomen\u001b[39m\u001b[38;5;124m'\u001b[39m, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilar words to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwomen\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilar_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mean_vector(keys, weight, pre_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, post_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'women' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Find words similar to the word women\n",
    "similar_words = model.wv.most_similar('women', topn=5)\n",
    "print(f\"Similar words to 'women': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dac2610-0743-466b-9d5c-7dcddf8d7c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'woman': [('men', 0.5933540463447571), ('female', 0.58524489402771), ('sex', 0.5548138618469238), ('wife', 0.5384922623634338), ('girl', 0.5375543236732483), ('unmarried', 0.5127474665641785), ('bisexual', 0.5066877007484436), ('inspectorinspectorsergeantconstabletotal', 0.5064346194267273), ('ranksavon', 0.484653502702713), ('mother', 0.47149765491485596), ('lesbian', 0.45528358221054077), ('sexual', 0.45225992798805237), ('adult', 0.44631966948509216), ('rankstotal', 0.44467100501060486), ('male', 0.44349783658981323)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=15)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5931ff-45ab-4bbb-9bf2-84e90d14b37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'man': [('men', 0.6600674986839294), ('someone', 0.5912762880325317), ('husband', 0.5009009838104248), ('person', 0.4972824454307556), ('wife', 0.49441832304000854)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c1abfd-9996-4712-bd27-0dd271c4c080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'man': [('men', 0.6600674986839294), ('someone', 0.5912762880325317), ('husband', 0.5009009838104248), ('person', 0.4972824454307556), ('wife', 0.49441832304000854), ('father', 0.4914131760597229), ('lad', 0.48043251037597656), ('somebody', 0.4727700650691986), ('people', 0.4538191556930542), ('chap', 0.4463156461715698), ('policeman', 0.44304999709129333), ('son', 0.4408462345600128), ('englishman', 0.4390304684638977), ('woman', 0.43529239296913147), ('fellow', 0.42490649223327637)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=15)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db35396f-64b2-4fc4-9856-00d0824057af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2b2cf27-0c44-4582-8267-5b131e55d4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'suffrage': [('franchise', 0.696742594242096), ('enfranchisement', 0.45555379986763), ('disfranchisement', 0.4372865557670593), ('voter', 0.3991124927997589), ('elective', 0.39418527483940125), ('elector', 0.38717013597488403), ('voting', 0.383173406124115), ('electorate', 0.3795275390148163), ('ballot', 0.37519317865371704), ('democratic', 0.37162959575653076), ('electoral', 0.37135225534439087), ('enfranchising', 0.3685806393623352), ('democracy', 0.35439103841781616), ('enfranchised', 0.3541511297225952), ('universal', 0.3528963327407837)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word suffrage\n",
    "similar_words = model.wv.most_similar('suffrage', topn=15)\n",
    "print(f\"Similar words to 'suffrage': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc8934a-71e5-4b46-aebc-a860991e7116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'equality': [('equal', 0.680004894733429), ('discrimination', 0.5354317426681519), ('parity', 0.4712681770324707), ('racial', 0.4699332118034363), ('gender', 0.4354051649570465), ('inequality', 0.4241330623626709), ('sex', 0.4130224585533142), ('equalityasked', 0.4092582166194916), ('footing', 0.3956541419029236), ('toleration', 0.3927280008792877), ('race', 0.3723876178264618), ('fairness', 0.3709823191165924), ('discriminated', 0.35953977704048157), ('diversity', 0.35619059205055237), ('differentiation', 0.3544129729270935)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('equality', topn=15)\n",
    "print(f\"Similar words to 'equality': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daafde53-27f7-4fde-9425-c9e208c91fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'woman': [ 2.3564      3.753244   -0.41304144  6.244112    1.8507706   3.714436\n",
      " -1.8692195   4.238472    1.4238366  -2.0187404  -3.2061436  -0.24064855\n",
      "  0.02811576 -4.186275    0.8700946   1.151019    1.4331686  -0.30517128\n",
      "  5.8615856   1.8697482   1.3613605  -4.4436474  -0.5636993  -3.880515\n",
      "  0.8894953   0.3649068   0.09802148 -0.93517095  4.601683   -0.02369897\n",
      "  0.60239774  1.0861696  -0.16868971  0.1900983  -3.9240632  -2.173769\n",
      "  0.9053637   0.08313857  4.0664454   4.8482037   0.22638184  0.49103066\n",
      " -0.08679341 -2.514798    2.0976393  -3.2160828   2.7779095   2.8058639\n",
      "  2.4393306  -0.74029464 -2.4814892   1.3102283   1.3731905   0.6360431\n",
      " -7.1056166  -2.0939715   0.49667063 -2.2025764   0.9198766  -2.8030362\n",
      " -0.89481515  5.537762    2.8655074   4.8633866  -0.18163304  0.08982263\n",
      "  1.8840882  -1.7859993  -3.1925774  -1.3498391   2.1551225  -0.7668172\n",
      "  0.883345   -1.8156962   2.731256    4.006288    1.1115992  -2.3946779\n",
      "  2.2193987   0.25998405 -0.49836123 -0.8223859   0.29111883 -0.17109515\n",
      " -1.4438462   1.4990792  -1.5142578  -1.3965831   1.3241632  -0.9389012\n",
      " -0.43425667 -0.57057804  4.5699053   3.1360068   0.0398838   0.27046943\n",
      "  1.8831233  -2.630019    4.852904   -1.4518026  -3.1939228   3.709996\n",
      " -4.2542377  -3.1342854  -1.0148206   0.10843506 -1.2889508   0.33063176\n",
      " -3.4356995  -2.0182247   3.8323405   1.3165804   3.7220175   2.6525228\n",
      "  2.4932582  -1.8205353   2.340731    0.1853259   5.0768948   4.135717\n",
      "  1.7867383  -0.16021875 -0.9867292   2.0197272   5.508968    1.0328752\n",
      " -0.44485798 -3.7912905   0.7923684   2.7655964   6.9856706   3.7088552\n",
      "  0.07948062 -0.7666807   1.5680258   0.67333406  4.1313715  -1.0628252\n",
      " -2.7815363  -0.01917161  0.3891008   0.48105848 -2.5395322  -3.508695\n",
      "  0.84983593  1.4521654  -3.1225278  -1.0326866  -2.729612    1.2632713\n",
      "  1.570045    0.12536378  0.7412067  -1.9594082   3.2004628  -2.3857152\n",
      " -1.693455    3.060438   -1.9588659   1.779757   -2.714381    2.6430886\n",
      " -1.0555574  -5.2032967   0.1407115  -3.003359    0.8817322   2.1332326\n",
      " -1.3819737  -0.26251814  0.82630235 -1.9630834  -2.361264   -0.8788648\n",
      "  0.65387744 -0.35751083 -2.8328571  -0.25924027  3.7839575   2.5007088\n",
      "  1.9036785   0.37967256  4.8576403   1.8310965   3.9300563   4.488299\n",
      " -0.4286056   4.384608    1.9761693  -2.7965827   0.5947647  -1.7804548\n",
      " -0.2203931  -0.70285493 -2.9501464   2.0636647  -0.5616119   1.1373912\n",
      "  0.26074582 -0.01076945 -1.0850145   6.7665954   5.3066187  -1.1685909\n",
      " -1.9799261  -2.6408265   1.0159787  -0.46034503  0.70402336  0.801694\n",
      " -1.8332932   1.2020165   1.1083598  -0.349839    1.6999408  -1.1282876\n",
      " -0.5211606   3.0138006  -0.7132729   0.0308491   2.6947415   3.4958813\n",
      "  3.4336116  -2.4066727  -1.8057292   0.46818268 -0.94070256 -1.7786087\n",
      "  3.9973323   1.9349685   4.43838    -0.5763218  -0.47116548  2.1592712\n",
      " -0.37291822 -1.9443412  -4.150662   -4.4563546  -1.8977883   2.7776282\n",
      " -0.6431565  -0.37705013  2.6673996  -1.5204972   2.1276586   2.3196282\n",
      "  0.29118425  2.4053202  -2.694574   -0.68372446  2.9794655   0.25896552\n",
      "  2.7763782  -3.660747    1.1561975  -0.8988582   0.9691146  -0.8341643\n",
      "  1.6576557   1.746163   -1.0999084   1.5986912   2.6622279   0.4706448\n",
      "  3.2847412   2.0894732  -4.260733    0.04558463  1.8029902   3.307366\n",
      " -2.033939   -1.4739326  -0.10251776  2.4084356  -1.761387    0.8321622\n",
      " -1.0106909   0.9777128  -2.3742554  -2.526338    3.277589   -3.787043\n",
      " -4.664528   -0.8885455  -4.2956414   1.2962378  -0.57281625  1.1741773\n",
      " -0.8211524  -1.5298262  -1.8392571  -2.2070816   0.39626762  2.8604202\n",
      " -5.309493   -0.44613037 -1.2327089   1.1121603   4.601772   -1.2051393 ]\n"
     ]
    }
   ],
   "source": [
    "#the vector representation of the word woman\n",
    "vector = model.wv['woman']\n",
    "print(f\"Vector representation of 'woman': {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c705332e-7d5a-4dc0-a08f-86874f4786f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('queen', 0.47559547424316406)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74b0f724-42bf-4f4e-8a72-c04438a54f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('mother', 0.5810819864273071)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1353424d-4d98-420c-b393-c8dc9178cab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('friend', 0.5325562357902527)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['gentleman', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcf03a81-2631-4baa-96db-fc7c754f923e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('dutiful', 0.38651522994041443)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['loyal', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb3e553e-52e2-4de5-bbe5-8300b5f53717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Visualize embeddings of 'man' and 'woman'\n",
    "words_to_visualize = ['man', 'woman']\n",
    "vectors_to_visualize = np.array([model.wv[word] for word in words_to_visualize])\n",
    "\n",
    "# Check the number of samples\n",
    "n_samples, n_features = vectors_to_visualize.shape\n",
    "\n",
    "# Ensuring perplexity is less than the number of samples\n",
    "perplexity = min(3, n_samples - 1)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "vectors_2d = tsne.fit_transform(vectors_to_visualize)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.xlim(vectors_2d[:, 0].min() - 0.1, vectors_2d[:, 0].max() + 0.1)\n",
    "plt.ylim(vectors_2d[:, 1].min() - 0.1, vectors_2d[:, 1].max() + 0.1)\n",
    "\n",
    "for i, word in enumerate(words_to_visualize):\n",
    "    plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])\n",
    "    plt.text(vectors_2d[i, 0] + 0.02, vectors_2d[i, 1] + 0.02, word)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372b3b0-2fdb-49a1-a4d4-9d6ce9916f63",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee75dbd-9a88-4924-acfb-2227b3313053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Iterate through each file in the directory\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_directory):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     14\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_directory, filename)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "\n",
    "# the directory path of tokenized files\n",
    "data_directory = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates'\n",
    "\n",
    "# Initialize an empty list to store sentences\n",
    "sentences = []\n",
    "print(\"here\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Read the tokenized data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_sentences = [line.split() for line in file]\n",
    "\n",
    "        # Add the sentences from this file to the overall list\n",
    "        sentences.extend(file_sentences)\n",
    "        \n",
    "        print(f\"Processed {len(file_sentences)} sentences from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556c93f-41bf-45a4-bcef-2c35ab66974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastText model...\n"
     ]
    }
   ],
   "source": [
    "# Print statement for indicating the start of training\n",
    "print(\"Training FastText model...\")\n",
    "\n",
    "# Train FastText model\n",
    "model = fasttext.train_unsupervised(sentences, model='skipgram', dim=300, ws=15, minCount=5, thread=4)\n",
    "\n",
    "# Print statement for the completion of training\n",
    "print(\"FastText model trained successfully.\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_model('fasttext_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc753b86-af74-45cf-a7e3-6510eb8caecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt\n",
      "Training FastText model...\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# the path to the tokenized file\n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "\n",
    "# Print statement for indicating the start of training\n",
    "print(\"Training FastText model...\")\n",
    "\n",
    "# Train FastText model\n",
    "model = fasttext.train_unsupervised(sentences, model='skipgram',dim=100, ws=5, minCount=1, thread=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save_model('fasttext_model.bin')\n",
    "\n",
    "\n",
    "# Print statement for indicating the completion of training\n",
    "print(\"FastText model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64a0ea-a2f3-40c5-b3c3-88f18a18ff7b",
   "metadata": {},
   "source": [
    "# Word2Vec for the year 1800-1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73105c09-d8c0-494b-8dce-362037745fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt\n",
      "Processed 471034 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt\n",
      "Word2Vec Model training completed\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "#the path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1800_1850_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_1.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1294bc-5a62-4be3-a2d6-11c8a5052b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8c2e88-a410-4332-8977-d972679def30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'woman': [('wife', 0.6032223105430603), ('female', 0.5838871002197266), ('girl', 0.5633318424224854), ('husband', 0.552932858467102), ('seducer', 0.5423462986946106)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3aae1d-3c8b-40e0-86d3-0e4af68f6f55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'man': [('men', 0.5075037479400635), ('hypocrite', 0.4152478277683258), ('creature', 0.4150362014770508), ('upright', 0.3986075222492218), ('falshoods', 0.3948286473751068)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e34845-e08c-4793-b5a7-35bfc87cdd04",
   "metadata": {},
   "source": [
    "# Word2Vec for the year 1851-1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54902e06-cb5f-4aed-8015-fabf5abd5a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1851_1900_tokenized.txt\n",
      "Processed 687819 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1851_1900_tokenized.txt\n",
      "Word2Vec Model training completed\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "# the path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1851_1900_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_2.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b541db4f-80c6-4b1a-ada0-2d47f399321c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03839a6-3c2c-43dd-99b2-da411ee290e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'woman': [('sex', 0.6303134560585022), ('female', 0.5213927626609802), ('wife', 0.5095798373222351), ('girl', 0.46941661834716797), ('unmarried', 0.45637622475624084)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b8e7168-e45a-4e1a-9136-391acbc8fcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'man': [('men', 0.6013084650039673), ('person', 0.5034669041633606), ('fool', 0.4483599066734314), ('policeman', 0.44412925839424133), ('neighbour', 0.43825453519821167)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc32a1-587b-4c43-bfb6-a6fc6a027b97",
   "metadata": {},
   "source": [
    "# Word2Vec for the year 1901-1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292b5f0d-8551-46e7-9fea-a98ce2ec3ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1901_1950_tokenized.txt\n",
      "Processed 765422 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1901_1950_tokenized.txt\n",
      "Word2Vec Model training completed\n",
      "Similar words to 'woman': [('unmarried', 0.5150774121284485), ('men', 0.4896622896194458), ('sex', 0.4873221218585968), ('wife', 0.48091354966163635), ('adult', 0.48029235005378723)]\n",
      "Similar words to 'man': [('men', 0.5892910361289978), ('husband', 0.48411449790000916), ('person', 0.43265873193740845), ('kicked', 0.4302557706832886), ('son', 0.4296678602695465)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "#the path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1901_1950_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_3.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")\n",
    "# Load the saved Word2Vec model\n",
    "model = model = Word2Vec.load('word2vec_model_3.bin')\n",
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")\n",
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfaa6b-5a51-4ffb-906f-63a3abd9cc84",
   "metadata": {},
   "source": [
    "# Word2Vec for the year  1951-1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d01861a-a217-4b40-b279-8f9a98e0fefd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1951_1970_tokenized.txt\n",
      "Processed 813361 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1951_1970_tokenized.txt\n",
      "Word2Vec Model training completed\n",
      "Similar words to 'woman': [('wife', 0.46793100237846375), ('men', 0.45501989126205444), ('sex', 0.4433596134185791), ('pregnant', 0.43973055481910706), ('girl', 0.4343293607234955)]\n",
      "Similar words to 'man': [('men', 0.618402361869812), ('lad', 0.5547232627868652), ('chap', 0.5354734063148499), ('someone', 0.5308980941772461), ('wife', 0.5179865956306458)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "#the path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1951_1970_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_4.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")\n",
    "# Load the saved Word2Vec model\n",
    "model = model = Word2Vec.load('word2vec_model_4.bin')\n",
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")\n",
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49727c3-cd56-48ab-8555-6c6c76e96bc8",
   "metadata": {},
   "source": [
    "# Word2Vec for the year 1971-1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25069508-b51c-469c-b591-70b04c6e5c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1971_1990_tokenized.txt\n",
      "Processed 1025994 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1971_1990_tokenized.txt\n",
      "Word2Vec Model training completed\n",
      "Similar words to 'woman': [('men', 0.5664160847663879), ('mother', 0.5024280548095703), ('pregnant', 0.45058295130729675), ('inspectorinspectorsergeantconstabletotal', 0.445293128490448), ('marry', 0.44471269845962524)]\n",
      "Similar words to 'man': [('men', 0.609752357006073), ('someone', 0.5387611389160156), ('chap', 0.5370227098464966), ('wife', 0.5013186931610107), ('somebody', 0.4577217996120453)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "#the path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1971_1990_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_5.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")\n",
    "# Load the saved Word2Vec model\n",
    "model = model = Word2Vec.load('word2vec_model_5.bin')\n",
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")\n",
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f611d-6283-4217-b80a-44133ce0aba5",
   "metadata": {},
   "source": [
    "# Word2Vec for the year 1991-2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b03ef1f-ddb3-4318-962e-148af5275a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1991_2006_tokenized.txt\n",
      "Processed 597305 sentences from /Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1991_2006_tokenized.txt\n",
      "Word2Vec Model training completed\n",
      "Similar words to 'woman': [('men', 0.5860161781311035), ('bisexual', 0.5489916205406189), ('gay', 0.47022512555122375), ('childbirth', 0.4553225636482239), ('pregnant', 0.44972044229507446)]\n",
      "Similar words to 'man': [('men', 0.5769185423851013), ('daughter', 0.5054463744163513), ('someone', 0.49621790647506714), ('raped', 0.4794258177280426), ('son', 0.4751015603542328)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "# The path to the tokenized file \n",
    "file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_debates/1991_2006_tokenized.txt'\n",
    "\n",
    "# Print statement for indicating the processing of the file\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the tokenized data from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences = [line.split() for line in file]\n",
    "    print(f\"Processed {len(sentences)} sentences from {file_path}\")\n",
    "\n",
    "    \n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=15, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('word2vec_model_6.bin')\n",
    "\n",
    "print(\"Word2Vec Model training completed\")\n",
    "# Load the saved Word2Vec model\n",
    "model = model = Word2Vec.load('word2vec_model_6.bin')\n",
    "# Find words similar to the word woman\n",
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(f\"Similar words to 'woman': {similar_words}\")\n",
    "# Find words similar to the word man\n",
    "similar_words = model.wv.most_similar('man', topn=5)\n",
    "print(f\"Similar words to 'man': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fde2435b-5951-46bf-82dd-762722b55f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('somebody', 0.3665020167827606)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['someone', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee85904-8f6a-47bc-bea1-72d72615c136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('daughter', 0.3618466258049011)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ca9e9-df9b-4c7c-aa4c-9ad2024048b9",
   "metadata": {},
   "source": [
    "# Word Analogy differences between all the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df84b8a1-c201-47d1-9dee-51c3259b5fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1850 = Word2Vec.load('word2vec_model_1.bin')\n",
    "model_1900 = Word2Vec.load('word2vec_model_2.bin')\n",
    "model_1950 = Word2Vec.load('word2vec_model_3.bin')\n",
    "model_1970 = Word2Vec.load('word2vec_model_4.bin')\n",
    "model_1990 = Word2Vec.load('word2vec_model_5.bin')\n",
    "model_2006 = Word2Vec.load('word2vec_model_6.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6d17b-8905-4550-a326-786341fdf96d",
   "metadata": {},
   "source": [
    "# Association of woman to childbirth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73eb5c-3b40-4aa7-a151-92c35b92c7cb",
   "metadata": {},
   "source": [
    "## For the year 1800-1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce57d2ab-d357-40fe-9bf5-5e887c12759e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('men', 0.36237356066703796)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1850.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cf3405f-d732-462b-9fec-b4c3d06802fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('administrator', 0.29424118995666504)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1900.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9f64706-c103-414a-b154-2b9ad21d755b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('illness', 0.38594549894332886)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1950.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc7007b9-e2f4-4e5c-acba-7b3cd42a02c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('someone', 0.3920690715312958)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1970.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "effb7521-434e-44f5-851a-1b37b9ba37dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('unpublishable', 0.3387390971183777)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1990.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc8a0cec-272d-4039-9009-c9f82243e3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('daughter', 0.3618466258049011)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_2006.wv.most_similar(positive=['childbirth', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20e5a086-ce46-4647-9501-d0218d932621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('reformer', 0.35064569115638733)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1850.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f75fdf23-56f5-454d-abbe-985c6e1b490e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('franchise', 0.35935381054878235)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1900.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03bf5066-39fa-46b9-bdba-fb605506781a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('mcnaghten', 0.3100873529911041)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1950.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca2ede6c-dc07-43b3-899e-8880e3586003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('even', 0.32846373319625854)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1970.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42eda517-cbf4-4106-bba6-624ae1ddc858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('dictatorship', 0.3907819390296936)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_1990.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1e68633-acd9-4195-85ed-9c9f2c3a4c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('peterloo', 0.41413602232933044)]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = model_2006.wv.most_similar(positive=['suffrage', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3aa5a854-96ec-4ee6-83f2-5e28b6b20a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('female', 0.4533041715621948)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_1850.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4fe7945-7983-4102-948e-7bf5ae7960d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('female', 0.4420682489871979)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_1900.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0757a7a-9dbf-4da0-8dca-42b7c7db6142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('recruited', 0.4198893904685974)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_1950.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d203a55d-b9ef-4274-979d-02d6048a4dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('recruited', 0.4622421860694885)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_1970.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcf9a1c9-4c48-44a6-a7d8-4e87f3dba8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('inspectorinspectorsergeantconstabletotal', 0.449300616979599)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_1990.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af6bfd45-3702-4c58-9477-691c81ff06be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result: [('bisexual', 0.4602353870868683)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result = model_2006.wv.most_similar(positive=['men', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"Word analogy result: {analogy_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ce99bf9-7af2-4d0b-ab6a-d2d87b65649e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('statesman', 0.35491928458213806)]\n",
      "Word analogy result for the year 1851-1900: [('comrade', 0.38791826367378235)]\n",
      "Word analogy result for the year 1901-1950: [('lad', 0.49189355969429016)]\n",
      "Word analogy result for the year 1951-1970: [('lad', 0.46681398153305054)]\n",
      "Word analogy result for the year 1971-1990: [('soldier', 0.5077458620071411)]\n",
      "Word analogy result for the year 1991-2006: [('someone', 0.41598090529441833)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['men', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "972bac48-2195-47ec-8ee1-92b4c0a1f664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('men', 0.3966078460216522)]\n",
      "Word analogy result for the year 1851-1900: [('intellect', 0.40558910369873047)]\n",
      "Word analogy result for the year 1901-1950: [('person', 0.35921648144721985)]\n",
      "Word analogy result for the year 1951-1970: [('someone', 0.4721141457557678)]\n",
      "Word analogy result for the year 1971-1990: [('someone', 0.37518638372421265)]\n",
      "Word analogy result for the year 1991-2006: [('someone', 0.3883684575557709)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['sex', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8134c6a1-78c8-44db-8f3c-4037525d062f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('men', 0.34754329919815063)]\n",
      "Word analogy result for the year 1851-1900: [('men', 0.4129961431026459)]\n",
      "Word analogy result for the year 1901-1950: [('person', 0.393573522567749)]\n",
      "Word analogy result for the year 1951-1970: [('someone', 0.41688984632492065)]\n",
      "Word analogy result for the year 1971-1990: [('workingmarried', 0.43949946761131287)]\n",
      "Word analogy result for the year 1991-2006: [('male', 0.4257601201534271)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['female', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4983857-2f0a-438b-804a-1646134cd3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('upright', 0.34561818838119507)]\n",
      "Word analogy result for the year 1851-1900: [('husband', 0.4486161172389984)]\n",
      "Word analogy result for the year 1901-1950: [('husband', 0.4925774335861206)]\n",
      "Word analogy result for the year 1951-1970: [('husband', 0.5561220049858093)]\n",
      "Word analogy result for the year 1971-1990: [('father', 0.4927841126918793)]\n",
      "Word analogy result for the year 1991-2006: [('husband', 0.5647310018539429)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['wife', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "432fad57-10cb-4db9-899f-66ed381dab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('men', 0.3185313045978546)]\n",
      "Word analogy result for the year 1851-1900: [('shilling', 0.32045993208885193)]\n",
      "Word analogy result for the year 1901-1950: [('impunity', 0.316707968711853)]\n",
      "Word analogy result for the year 1951-1970: [('someone', 0.3898908197879791)]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'seducer' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord analogy result for the year 1951-1970: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalogy_result_1970\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#word analogy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m analogy_result_2006 \u001b[38;5;241m=\u001b[39m model_2006\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseducer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m'\u001b[39m], negative\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m'\u001b[39m], topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord analogy result for the year 1991-2006: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalogy_result_2006\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#word analogy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mean_vector(keys, weight, pre_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, post_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'seducer' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c20b9b06-0a61-4803-95d7-0f05c54d0071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'seducer' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#word analogy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m analogy_result_1990 \u001b[38;5;241m=\u001b[39m model_1990\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseducer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m'\u001b[39m], negative\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m'\u001b[39m], topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord analogy result for the year 1971-1990: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalogy_result_1990\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mean_vector(keys, weight, pre_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, post_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'seducer' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['seducer', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6b5e69d-7926-4526-b40e-d48fa77d646c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy result for the year 1800-1850: [('men', 0.40793514251708984)]\n",
      "Word analogy result for the year 1851-1900: [('boy', 0.5028549432754517)]\n",
      "Word analogy result for the year 1901-1950: [('boy', 0.5737881064414978)]\n",
      "Word analogy result for the year 1951-1970: [('boy', 0.5466696619987488)]\n",
      "Word analogy result for the year 1971-1990: [('boy', 0.5339813828468323)]\n",
      "Word analogy result for the year 1991-2006: [('boy', 0.5456141233444214)]\n"
     ]
    }
   ],
   "source": [
    "#word analogy\n",
    "analogy_result_1850 = model_1850.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1800-1850: {analogy_result_1850}\")\n",
    "#word analogy\n",
    "analogy_result_1900 = model_1900.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1851-1900: {analogy_result_1900}\")\n",
    "#word analogy\n",
    "analogy_result_1950 = model_1950.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1901-1950: {analogy_result_1950}\")\n",
    "#word analogy\n",
    "analogy_result_1970 = model_1970.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1951-1970: {analogy_result_1970}\")\n",
    "#word analogy\n",
    "analogy_result_1990 = model_1990.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1971-1990: {analogy_result_1990}\")\n",
    "#word analogy\n",
    "analogy_result_2006 = model_2006.wv.most_similar(positive=['girl', 'man'], negative=['woman'], topn=1)\n",
    "print(f\"Word analogy result for the year 1991-2006: {analogy_result_2006}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e5aa2-01d9-42ae-913b-802ab89c2c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
