{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a530a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shaistasyeda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shaistasyeda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shaistasyeda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f3a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec58a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_stopwords = set(stopwords.words('english')) | {\n",
    "    'he', 'she', 'it', 'they', 'we', 'you', 'I', 'me', 'us', 'them', \n",
    "    'him', 'her', 'his', 'hers', 'theirs', 'our', 'your', 'my', 'mine', \n",
    "    'yourself', 'myself', 'ourselves', 'yourselves', 'themselves', \n",
    "    'himself', 'herself', 'itself', 'who', 'whom', 'whose', 'which', 'that',\n",
    "    'this', 'these', 'those'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2364b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of known concatenated words\n",
    "concatenated_words_dict = {\n",
    "    'womenaged': 'women aged',\n",
    "    'placewomen': 'place women',\n",
    "    'workwomen' : 'work women',\n",
    "    'overwomen' : 'over women'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c59fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_concatenated_words(word, concatenated_dict):\n",
    "    return concatenated_dict.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ded853c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lemmatize_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace all forms of dashes and hyphens with a space\n",
    "    text = re.sub(r'[-–—]+', ' ', text)\n",
    "\n",
    "    # Separate numbers and words concatenated together\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Handle concatenated words, then lemmatize and remove stopwords\n",
    "    processed_tokens = [lemmatizer.lemmatize(handle_concatenated_words(word, concatenated_words_dict)) for word in tokens if word not in extended_stopwords]\n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = ' '.join(processed_tokens)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5afa912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 woman 20 men competition\n"
     ]
    }
   ],
   "source": [
    "text = \"There were 18women and 20men in the competition.\"\n",
    "clean_text = clean_and_lemmatize_text(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1da4e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = '/Users/shaistasyeda/Desktop/DataSet/merged-files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7a8af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "destination_directory = 'cleaned_files_1'\n",
    "os.makedirs(destination_directory, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3197403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as cleaned_files_1/cleaned_1984_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1890_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1919_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1858_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1998_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1905_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1932_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1811_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1826_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1967_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1950_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1873_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1844_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1855_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1862_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1941_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1976_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1837_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1923_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1914_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1989_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2001_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1849_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1908_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1881_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1995_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1898_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1973_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1944_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1867_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1850_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1911_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1926_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1805_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1832_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1990_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1884_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2004_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1958_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1819_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1808_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1949_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1895_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1981_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1823_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1937_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1900_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1841_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1876_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1955_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1962_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1889_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1930_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1907_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1824_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1813_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1952_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1965_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1846_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1871_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1986_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1892_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1838_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1979_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1968_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2003_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1883_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1997_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1860_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1857_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1974_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1943_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1835_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1916_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1921_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1992_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1886_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1879_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1938_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1946_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1971_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1852_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1865_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1924_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1913_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1830_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1807_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1821_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1902_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1935_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1874_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1843_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1960_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1957_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1929_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1868_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1897_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1983_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1972_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1945_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1866_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1851_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1910_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1927_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1804_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1833_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1899_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2005_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1959_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1818_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1991_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1885_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1894_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1980_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1809_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1948_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1888_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1822_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1815_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1936_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1901_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1840_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1877_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1954_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1963_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1918_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1859_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1985_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1891_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1904_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1933_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1810_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1827_merged.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as cleaned_files_1/cleaned_1966_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1951_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1872_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1845_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1999_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1988_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1854_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1863_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1940_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1977_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1836_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1922_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1915_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1880_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1994_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1848_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2000_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1909_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1878_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1939_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1993_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1887_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1947_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1970_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1853_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1864_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1925_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1912_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1831_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1806_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1817_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1820_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1903_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1934_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1875_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1842_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1961_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1956_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1896_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1013_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1982_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1928_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1869_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1931_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1906_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1825_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1812_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1953_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1964_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1847_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1870_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1839_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1978_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1987_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1893_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1882_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1996_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1969_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_2002_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1828_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1861_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1856_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1975_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1942_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1803_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1834_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1917_merged.txt\n",
      "Cleaned file saved as cleaned_files_1/cleaned_1920_merged.txt\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each file in the source directory\n",
    "for file_name in os.listdir(source_directory):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(source_directory, file_name)\n",
    "        \n",
    "        # Read the content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "        \n",
    "        # Clean the data\n",
    "        cleaned_data = clean_and_lemmatize_text(data)\n",
    "        \n",
    "        # Define the path for the cleaned file\n",
    "        cleaned_file_path = os.path.join(destination_directory, f\"cleaned_{file_name}\")\n",
    "        \n",
    "        # Save the cleaned data\n",
    "        with open(cleaned_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_data)\n",
    "        \n",
    "        print(f\"Cleaned file saved as {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19551c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b013f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shaistasyeda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1711df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_directory = 'cleaned_files_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54e8989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file_path = '/Users/shaistasyeda/Desktop/DataSet/combined_tokenized.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "723f92e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created at /Users/shaistasyeda/Desktop/DataSet/combined_tokenized.txt\n"
     ]
    }
   ],
   "source": [
    "# Combine all tokenized files into one large file\n",
    "with open(combined_file_path, 'w', encoding='utf-8') as combined_file:\n",
    "    for file_name in os.listdir(tokenized_directory):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(tokenized_directory, file_name)\n",
    "            \n",
    "            # Read each file and append its content to the combined file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                combined_file.write(file.read() + '\\n')\n",
    "\n",
    "print(f\"Combined file created at {combined_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14da643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 102M words\n",
      "Number of words:  99895\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   71167 lr:  0.000000 avg.loss:  0.443423 ETA:   0h 0m 0s 87.1% words/sec/thread:   71088 lr:  0.006437 avg.loss:  0.507168 ETA:   0h 2m12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText model trained and saved to /Users/shaistasyeda/Desktop/DataSet/fasttext_model_new.bin\n"
     ]
    }
   ],
   "source": [
    "# Train the fastText model on the combined file\n",
    "model = fasttext.train_unsupervised(combined_file_path, model='skipgram')\n",
    "# Save the trained model\n",
    "model_path = '/Users/shaistasyeda/Desktop/DataSet/fasttext_model_new.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"fastText model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d43e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7447376251220703 women\n",
      "0.7246029376983643 female\n",
      "0.694318950176239 marriedwomen\n",
      "0.689653217792511 men\n",
      "0.6832957863807678 womanhood\n",
      "0.6819168925285339 womenfolk\n",
      "0.6800893545150757 male\n",
      "0.6746634244918823 unmarried\n",
      "0.6728211641311646 adult\n",
      "0.6716204881668091 childbearing\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68f7bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5839092135429382 elizabeth\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "106a414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591094970703125 men\n",
      "0.7052745223045349 suppose\n",
      "0.6928899884223938 surely\n",
      "0.680347740650177 one\n",
      "0.6780292987823486 every\n",
      "0.6763904690742493 supposed\n",
      "0.6748271584510803 irishwoman\n",
      "0.6712268590927124 englishwoman\n",
      "0.6685264110565186 really\n",
      "0.6678933501243591 father\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "540f2e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 9M words\n",
      "Number of words:  22155\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   54857 lr:  0.000022 avg.loss:  2.069668 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved to fasttext_model_1849.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 100.0% words/sec/thread:   54842 lr: -0.000008 avg.loss:  2.069920 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:   54842 lr:  0.000000 avg.loss:  2.069920 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "text_file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_files_1/merged_1800_1849.txt'\n",
    "\n",
    "# Train a fastText model\n",
    "model = fasttext.train_unsupervised(text_file_path, model='skipgram')\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'fasttext_model_1849.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"Model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a21b21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6953782439231873 child\n",
      "0.6842429041862488 husbandman\n",
      "0.6666134595870972 scotchwoman\n",
      "0.6374983787536621 womb\n",
      "0.6339884996414185 girl\n",
      "0.6255537271499634 husband\n",
      "0.6230942010879517 young\n",
      "0.6200757026672363 husbanded\n",
      "0.6046341061592102 female\n",
      "0.6040182113647461 trowsers\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa9aba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890785336494446 never\n",
      "0.7784768342971802 ever\n",
      "0.770086407661438 could\n",
      "0.737967312335968 thing\n",
      "0.7307202816009521 say\n",
      "0.7187234163284302 men\n",
      "0.7100488543510437 really\n",
      "0.7014883756637573 one\n",
      "0.698104739189148 indeed\n",
      "0.6849561929702759 every\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "645b4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074321269989014 anne\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3576dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 23M words\n",
      "Number of words:  34127\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   51643 lr:  0.000000 avg.loss:  1.203126 ETA:   0h 0m 0s 54.4% words/sec/thread:   50689 lr:  0.022804 avg.loss:  1.876559 ETA:   0h 2m31s 60.9% words/sec/thread:   50981 lr:  0.019575 avg.loss:  1.761130 ETA:   0h 2m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved to fasttext_model_1920.bin\n"
     ]
    }
   ],
   "source": [
    "text_file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_files_1/merged_1850_1920.txt'\n",
    "\n",
    "# Train a fastText model\n",
    "model = fasttext.train_unsupervised(text_file_path, model='skipgram')\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'fasttext_model_1920.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"Model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b74577",
   "metadata": {},
   "source": [
    "# suffragette means a woman seeking the right to vote through organized protest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cb05909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7199098467826843 fiancées\n",
      "0.6916109919548035 women\n",
      "0.6591169238090515 married\n",
      "0.655926525592804 female\n",
      "0.644893229007721 irishwoman\n",
      "0.6409128904342651 worker\n",
      "0.6236534714698792 person\n",
      "0.6235905885696411 man\n",
      "0.6221939921379089 suffragette\n",
      "0.6192792654037476 adult\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ceb0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7790971994400024 men\n",
      "0.7223900556564331 person\n",
      "0.694073498249054 hireling\n",
      "0.6855025291442871 every\n",
      "0.66920405626297 licked\n",
      "0.6661655306816101 surely\n",
      "0.6633811593055725 case\n",
      "0.6503986120223999 many\n",
      "0.6449174880981445 one\n",
      "0.644869863986969 picked\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa74eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5916923880577087 anne\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6876c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 27M words\n",
      "Number of words:  41684\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   51354 lr:  0.000000 avg.loss:  1.035489 ETA:   0h 0m 0s 53.1% words/sec/thread:   51302 lr:  0.023449 avg.loss:  1.666435 ETA:   0h 3m 0s 1.483058 ETA:   0h 2m27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved to fasttext_model_1970.bin\n"
     ]
    }
   ],
   "source": [
    "text_file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_files_1/merged_1921_1970.txt'\n",
    "\n",
    "# Train a fastText model\n",
    "model = fasttext.train_unsupervised(text_file_path, model='skipgram')\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'fasttext_model_1970.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"Model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71ecd",
   "metadata": {},
   "source": [
    "# Charwoman means a woman employed to clean houses or offices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7c44a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669793367385864 men\n",
      "0.727089524269104 charwoman\n",
      "0.7151160836219788 young\n",
      "0.6847538948059082 childless\n",
      "0.6748203635215759 policewoman\n",
      "0.6726569533348083 womanly\n",
      "0.6689029335975647 unmarried\n",
      "0.6667106747627258 wife\n",
      "0.6663983464241028 man\n",
      "0.6653931736946106 female\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5209cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702977657318115 men\n",
      "0.6844482421875 charwoman\n",
      "0.6663983464241028 woman\n",
      "0.6410592794418335 breadwinner\n",
      "0.6375285983085632 person\n",
      "0.6359814405441284 wife\n",
      "0.6223604083061218 people\n",
      "0.6214350461959839 wronged\n",
      "0.6196485757827759 lad\n",
      "0.6195102334022522 son\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bff5e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5752031207084656 mary\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d72f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 24M words\n",
      "Number of words:  54967\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   49060 lr:  0.000000 avg.loss:  1.087068 ETA:   0h 0m 0s  5.2% words/sec/thread:   50070 lr:  0.047409 avg.loss:  1.713953 ETA:   0h 5m32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved to fasttext_model_1990.bin\n"
     ]
    }
   ],
   "source": [
    "text_file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_files_1/merged_1971_1990.txt'\n",
    "\n",
    "# Train a fastText model\n",
    "model = fasttext.train_unsupervised(text_file_path, model='skipgram')\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'fasttext_model_1990.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"Model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34baf187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7491626143455505 men\n",
      "0.670244038105011 childbearing\n",
      "0.663090705871582 marriedwomen\n",
      "0.6365087628364563 teenager\n",
      "0.6146580576896667 wife\n",
      "0.6042739748954773 employable\n",
      "0.600101888179779 female\n",
      "0.5949774384498596 widower\n",
      "0.5892146229743958 youngest\n",
      "0.5888279676437378 married\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab5717ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078957557678223 one\n",
      "0.6921384334564209 men\n",
      "0.642246425151825 languishing\n",
      "0.6334478855133057 every\n",
      "0.6250906586647034 never\n",
      "0.6244341731071472 someone\n",
      "0.6221234798431396 birching\n",
      "0.6214703321456909 peep\n",
      "0.6199305057525635 stepfather\n",
      "0.6188099384307861 probably\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52691033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5924639105796814 elizabeth\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "251026c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 17M words\n",
      "Number of words:  45885\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   47036 lr:  0.000000 avg.loss:  1.388192 ETA:   0h 0m 0s% words/sec/thread:   46932 lr:  0.005058 avg.loss:  1.511997 ETA:   0h 0m26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved to fasttext_model_2006.bin\n"
     ]
    }
   ],
   "source": [
    "text_file_path = '/Users/shaistasyeda/Desktop/DataSet/tokenized_files_1/merged_1991_2006.txt'\n",
    "\n",
    "# Train a fastText model\n",
    "model = fasttext.train_unsupervised(text_file_path, model='skipgram')\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'fasttext_model_2006.bin'\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"Model trained and saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fcc8b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8254725933074951 men\n",
      "0.696035623550415 pregnant\n",
      "0.6881476044654846 female\n",
      "0.6872449517250061 wwomen\n",
      "0.6670705080032349 male\n",
      "0.6598854064941406 womb\n",
      "0.6541346311569214 wfemale\n",
      "0.653569221496582 bisexual\n",
      "0.6501618027687073 lesbian\n",
      "0.6339027285575867 sex\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('woman', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "316dae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6504823565483093 men\n",
      "0.6124760508537292 couple\n",
      "0.607006311416626 le\n",
      "0.5958391427993774 wife\n",
      "0.5943895578384399 aged\n",
      "0.588749885559082 manger\n",
      "0.5874067544937134 deserted\n",
      "0.577562153339386 adamant\n",
      "0.5726186037063599 shilling\n",
      "0.5696030259132385 father\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.get_nearest_neighbors('man', k=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1b7f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6307041049003601 queensgate\n"
     ]
    }
   ],
   "source": [
    "analogy = model.get_analogies('king', 'man', 'queen', k=1)\n",
    "for word, similarity in analogy:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793cd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
